{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Optimization objective\n",
    "\n",
    "The KL divergence between the variational distribution $q(\\mathbf{w} \\lvert \\boldsymbol{\\theta})$ and the true posterior $p(\\mathbf{w} \\lvert \\mathcal{D})$ is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{KL}(q(\\mathbf{w} \\lvert \\boldsymbol{\\theta}) \\mid\\mid p(\\mathbf{w} \\lvert \\mathcal{D})) &=\n",
    "\\int{q(\\mathbf{w} \\lvert \\boldsymbol{\\theta}) \\log \n",
    "    {{q(\\mathbf{w} \\lvert \\boldsymbol{\\theta})} \\over {p(\\mathbf{w} \\lvert \\mathcal{D})}} d\\mathbf{w}} \\\\\\\\ &=\n",
    "\\mathbb{E}_{q(\\mathbf{w} \\lvert \\boldsymbol{\\theta})} \\log\n",
    "    {{q(\\mathbf{w} \\lvert \\boldsymbol{\\theta})} \\over {p(\\mathbf{w} \\lvert \\mathcal{D})}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Applying Bayes' rule to $p(\\mathbf{w} \\lvert \\mathcal{D})$ we obtain\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{KL}(q(\\mathbf{w} \\lvert \\boldsymbol{\\theta}) \\mid\\mid p(\\mathbf{w} \\lvert \\mathcal{D})) &=\n",
    "\\mathbb{E}_{q(\\mathbf{w} \\lvert \\boldsymbol{\\theta})} \\log\n",
    "    {{q(\\mathbf{w} \\lvert \\boldsymbol{\\theta})} \\over \n",
    "     {p(\\mathcal{D} \\lvert \\mathbf{w}) p(\\mathbf{w})}} p(\\mathcal{D}) \\\\\\\\ &=\n",
    "\\mathbb{E}_{q(\\mathbf{w} \\lvert \\boldsymbol{\\theta})} \\left[\n",
    "    \\log q(\\mathbf{w} \\lvert \\boldsymbol{\\theta}) - \n",
    "    \\log p(\\mathcal{D} \\lvert \\mathbf{w}) - \n",
    "    \\log p(\\mathbf{w}) + \n",
    "    \\log p(\\mathcal{D}) \n",
    "\\right] \\\\\\\\ &=\n",
    "\\mathbb{E}_{q(\\mathbf{w} \\lvert \\boldsymbol{\\theta})} \\left[\n",
    "    \\log q(\\mathbf{w} \\lvert \\boldsymbol{\\theta}) - \n",
    "    \\log p(\\mathcal{D} \\lvert \\mathbf{w}) - \n",
    "    \\log p(\\mathbf{w})\n",
    "\\right] + \\log p(\\mathcal{D}) \\\\\\\\ &=\n",
    "\\mathrm{KL}(q(\\mathbf{w} \\lvert \\boldsymbol{\\theta}) \\mid\\mid q(\\mathbf{w})) - \n",
    "\\mathbb{E}_{q(\\mathbf{w} \\lvert \\boldsymbol{\\theta})} \\log p(\\mathcal{D} \\lvert \\mathbf{w}) +\n",
    "\\log p(\\mathcal{D})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "using the fact that the *log marginal likelihood* $\\log p(\\mathcal{D})$ doesn't depend on $\\mathbf{w}$. The first two terms on the RHS are the *variational free energy* $\\mathcal{F}(\\mathcal{D},\\boldsymbol{\\theta})$ as defined in Eq. $(1)$. We obtain \n",
    "\n",
    "$$\n",
    "\\mathrm{KL}(q(\\mathbf{w} \\lvert \\boldsymbol{\\theta}) \\mid\\mid p(\\mathbf{w} \\lvert \\mathcal{D})) =\n",
    "\\mathcal{F}(\\mathcal{D},\\boldsymbol{\\theta}) + \\log p(\\mathcal{D})\n",
    "$$\n",
    "\n",
    "In order to minimize $\\mathrm{KL}(q(\\mathbf{w} \\lvert \\boldsymbol{\\theta}) \\mid\\mid p(\\mathbf{w} \\lvert \\mathcal{D}))$ w.r.t. $\\boldsymbol{\\theta}$ we only need to minimize $\\mathcal{F}(\\mathcal{D},\\boldsymbol{\\theta})$ as $p(\\mathcal{D})$ doesn't depend on $\\boldsymbol{\\theta}$. The negative variational free energy is also known as *evidence lower bound* $\\mathcal{L}(\\mathcal{D},\\boldsymbol{\\theta})$ (ELBO). \n",
    "\n",
    "$$\n",
    "\\mathrm{KL}(q(\\mathbf{w} \\lvert \\boldsymbol{\\theta}) \\mid\\mid p(\\mathbf{w} \\lvert \\mathcal{D})) =\n",
    "-\\mathcal{L}(\\mathcal{D},\\boldsymbol{\\theta}) + \\log p(\\mathcal{D})\n",
    "$$\n",
    "\n",
    "It is a lower bound on $\\log p(\\mathcal{D})$ because the Kullback-Leibler divergence is always non-negative.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\mathcal{D},\\boldsymbol{\\theta}) &= \n",
    "\\log p(\\mathcal{D}) - \\mathrm{KL}(q(\\mathbf{w} \\lvert \\boldsymbol{\\theta}) \\mid\\mid p(\\mathbf{w} \\lvert \\mathcal{D})) \\\\\\\\\n",
    "\\mathcal{L}(\\mathcal{D},\\boldsymbol{\\theta}) &\\leq\n",
    "\\log p(\\mathcal{D})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore, the KL divergence between the variational distribution $q(\\mathbf{w} \\lvert \\boldsymbol{\\theta})$ and the true posterior $p(\\mathbf{w} \\lvert \\mathcal{D})$ is also minimized by maximizing the evidence lower bound."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
